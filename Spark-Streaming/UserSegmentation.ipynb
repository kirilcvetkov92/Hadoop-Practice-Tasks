{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-gqhSeeBHUmM"
   },
   "source": [
    "## Task 2. Heuristic user segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HgDnsASeHUmN"
   },
   "source": [
    "In this task you should firstly parse the user logs. Then distiguish segments and count the *unique* uids in each segment. Sort the output by counts.\n",
    "\n",
    "You may find more useful methods in the following sources:\n",
    "\n",
    "* Book \"Learning Spark: Lightning-Fast Big Data Analysis\" by Holden Karau.\n",
    "\n",
    "* [SparkStreaming Documentation](https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark-streaming-module)\n",
    "\n",
    "* [HyperLogLog documentation](https://pypi.org/project/hyperloglog/), [HyperLogLog theory](https://databricks.com/blog/2016/05/19/approximate-algorithms-in-apache-spark-hyperloglog-and-quantiles.html)\n",
    "\n",
    "* [Ua_Parser_documentation](https://pypi.org/project/ua-parser/0.7.0/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2fFVbsv1HUmP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from time import sleep\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# You need also use this specific libraries\n",
    "from ua_parser import user_agent_parser\n",
    "from hyperloglog import HyperLogLog\n",
    "from ua_parser import user_agent_parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QAVfv1zwHUmg"
   },
   "outputs": [],
   "source": [
    "# # Here is an example of `user_agent_parser`\n",
    "# from ua_parser import user_agent_parser\n",
    "# ua = 'Mozilla/5.0 (iPad; CPU OS 7_0_4 like Mac OS X) AppleWebKit/537.51.1 (KHTML, like Gecko) Version/7.0 Mobile/11B554a Safari/9537.53'\n",
    "# user_agent_parser.Parse(ua)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IxzrfYmLHUmr"
   },
   "source": [
    "**NB.** Please don't change the cell below. It is used for emulation realtime batch arriving. But figure out the code, it will help you when you'll work with real SparkStreaming applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1At3lil3HUms"
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(master='local[4]')\n",
    "\n",
    "# Preparing batches with the input data\n",
    "DATA_PATH = \"/data/course4/uid_ua_100k_splitted_by_5k\"\n",
    "batches = [sc.textFile(os.path.join(DATA_PATH, path)) for path in os.listdir(DATA_PATH)]\n",
    "\n",
    "# Creating Dstream to emulate realtime data generating\n",
    "BATCH_TIMEOUT = 5 # Timeout between the batch generation\n",
    "ssc = StreamingContext(sc, BATCH_TIMEOUT)\n",
    "dstream = ssc.queueStream(rdds=batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G9iAw3LiHUmw"
   },
   "source": [
    "There are 2 flags used in this task. \n",
    "* `finished` flag indicates if the current RDD is empty.\n",
    "* `printed` indicates the the result has been printed and SparkStreaming context can be stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1DX4zUteHUmy"
   },
   "outputs": [],
   "source": [
    "finished = False\n",
    "printed = False\n",
    "\n",
    "def set_ending_flag(rdd):\n",
    "    global finished\n",
    "    if rdd.isEmpty():\n",
    "        finished = True\n",
    "\n",
    "def print_only_at_the_end(rdd):\n",
    "    global printed\n",
    "    rdd.count()\n",
    "    if finished and not printed:\n",
    "\n",
    "        ans = rdd.sortBy(lambda x : x[1], ascending=False).collect()\n",
    "   \n",
    "        for key in ans:\n",
    "            print(\"seg_\"+key[0], key[1])\n",
    "             \n",
    "        printed = True\n",
    "\n",
    "# If we have received empty an rdd, the stream is finished.\n",
    "# So print the result and stop the context.\n",
    "\n",
    "dstream.foreachRDD(set_ending_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregator(values, old):\n",
    "    if not old :\n",
    "        old = HyperLogLog(0.01)\n",
    "    \n",
    "    for value in values : \n",
    "        old.add(value)\n",
    "        \n",
    "    return old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(line):\n",
    "    \n",
    "    line_split =  line.split(\"\\t\")\n",
    "    user_hash = line_split[0]\n",
    "    parsed_line = user_agent_parser.Parse(line_split[1])\n",
    "    \n",
    "    device = parsed_line['device']['family'].lower()\n",
    "\n",
    "    browser = parsed_line['user_agent']['family'].lower()\n",
    "\n",
    "    os = parsed_line['os']['family'].lower()\n",
    "    \n",
    "    list_tupples = []\n",
    "    \n",
    "    list_tupples.append((device, user_hash))\n",
    "    list_tupples.append((browser, user_hash))\n",
    "    list_tupples.append((os, user_hash))\n",
    "    \n",
    "    return list_tupples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_devices(filter_tuple):\n",
    "    word, _ = filter_tuple\n",
    "    return word==\"iphone\" or word==\"firefox\" or word==\"windows\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P7xaIlYtHUm3"
   },
   "outputs": [],
   "source": [
    "# Type your code for data processing and aggregation here\n",
    "\n",
    "\n",
    "dstream.flatMap(lambda line: preprocess(line))\\\n",
    "       .filter(lambda word : filter_devices(word))\\\n",
    "       .updateStateByKey(aggregator)\\\n",
    "       .map(lambda tupple : (tupple[0], len(tupple[1])))\\\n",
    "       .foreachRDD(print_only_at_the_end)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vO7omMKEHUnD"
   },
   "source": [
    "**NB.** Please don't change the cell below. It is used for stopping SparkStreaming context and Spark context when the stream finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "puSwVJshHUnG"
   },
   "outputs": [],
   "source": [
    "ssc.checkpoint('./checkpoint')  # checkpoint for storing current state        \n",
    "ssc.start()\n",
    "while not printed:\n",
    "    pass\n",
    "ssc.stop()  # when the result printed, stop the SparkStreaming context\n",
    "sc.stop()  # stop the Spark context to be able restart the code without restarting the kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GPMoQ7HyHUnJ"
   },
   "source": [
    "Here you can see the part of an output on the sample dataset:\n",
    "```\n",
    "...\n",
    "seg_unknown 22377\n",
    "seg_firefox 8237\n",
    "...\n",
    "```\n",
    "Of course, the numbers may be different but not very much (the error about 1% will be accepted)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "1002_flavor.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
