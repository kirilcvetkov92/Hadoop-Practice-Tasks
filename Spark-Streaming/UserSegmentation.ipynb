{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-gqhSeeBHUmM"
   },
   "source": [
    "## Task 2. Heuristic user segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HgDnsASeHUmN"
   },
   "source": [
    "In this task you should firstly parse the user logs. Then distiguish segments and count the *unique* uids in each segment. Sort the output by counts.\n",
    "\n",
    "You may find more useful methods in the following sources:\n",
    "\n",
    "* Book \"Learning Spark: Lightning-Fast Big Data Analysis\" by Holden Karau.\n",
    "\n",
    "* [SparkStreaming Documentation](https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark-streaming-module)\n",
    "\n",
    "* [HyperLogLog documentation](https://pypi.org/project/hyperloglog/), [HyperLogLog theory](https://databricks.com/blog/2016/05/19/approximate-algorithms-in-apache-spark-hyperloglog-and-quantiles.html)\n",
    "\n",
    "* [Ua_Parser_documentation](https://pypi.org/project/ua-parser/0.7.0/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2fFVbsv1HUmP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from time import sleep\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# You need also use this specific libraries\n",
    "from ua_parser import user_agent_parser\n",
    "from hyperloglog import HyperLogLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QAVfv1zwHUmg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'device': {'brand': 'Apple', 'family': 'iPad', 'model': 'iPad'},\n",
       " 'os': {'family': 'iOS',\n",
       "  'major': '7',\n",
       "  'minor': '0',\n",
       "  'patch': '4',\n",
       "  'patch_minor': None},\n",
       " 'string': 'Mozilla/5.0 (iPad; CPU OS 7_0_4 like Mac OS X) AppleWebKit/537.51.1 (KHTML, like Gecko) Version/7.0 Mobile/11B554a Safari/9537.53',\n",
       " 'user_agent': {'family': 'Mobile Safari',\n",
       "  'major': '7',\n",
       "  'minor': '0',\n",
       "  'patch': None}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is an example of `user_agent_parser`\n",
    "\n",
    "from ua_parser import user_agent_parser\n",
    "ua = 'Mozilla/5.0 (iPad; CPU OS 7_0_4 like Mac OS X) AppleWebKit/537.51.1 (KHTML, like Gecko) Version/7.0 Mobile/11B554a Safari/9537.53'\n",
    "user_agent_parser.Parse(ua)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IxzrfYmLHUmr"
   },
   "source": [
    "**NB.** Please don't change the cell below. It is used for emulation realtime batch arriving. But figure out the code, it will help you when you'll work with real SparkStreaming applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1At3lil3HUms"
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(master='local[4]')\n",
    "\n",
    "DATA_PATH = \"/data/course4/uid_ua_100k_splitted_by_5k\"\n",
    "batches = [sc.textFile(os.path.join(DATA_PATH, path)) for path in os.listdir(DATA_PATH)]\n",
    "\n",
    "BATCH_TIMEOUT = 5 # Timeout between batch generation\n",
    "ssc = StreamingContext(sc, BATCH_TIMEOUT)\n",
    "dstream = ssc.queueStream(rdds=batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G9iAw3LiHUmw"
   },
   "source": [
    "There are 2 flags used in this task. \n",
    "* `finished` flag indicates if the current RDD is empty.\n",
    "* `printed` indicates the the result has been printed and SparkStreaming context can be stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1DX4zUteHUmy"
   },
   "outputs": [],
   "source": [
    "finished = False\n",
    "printed = False\n",
    "\n",
    "def set_ending_flag(rdd):\n",
    "    global finished\n",
    "    if rdd.isEmpty():\n",
    "        finished = True\n",
    "\n",
    "def print_only_at_the_end(rdd):\n",
    "    global printed\n",
    "    rdd.count()\n",
    "    if finished and not printed:\n",
    "\n",
    "        # Type your code for sorting and printing the resulting RDD\n",
    "        ans = rdd.collect()\n",
    "        \n",
    "      \n",
    "        print(ans[2][0]+'\\t'+str(ans[2][1]))\n",
    "        print(ans[1][0]+'\\t'+str(ans[1][1]))\n",
    "        print(ans[0][0]+'\\t'+str(ans[0][1]))\n",
    "    \n",
    "        printed = True\n",
    "\n",
    "# If we have received empty an rdd, the stream is finished.\n",
    "# So print the result and stop the context.\n",
    "\n",
    "dstream.foreachRDD(set_ending_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregator(values, old):\n",
    "    return (old or 0) + sum(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hll = sc.broadcast(HyperLogLog(0.01))\n",
    "\n",
    "def filter_duplicate_user(line, broadcast_hll):\n",
    "    user_hash = line.split(\"\\t\")[0]\n",
    "    count = len(broadcast_hll.value)\n",
    "    broadcast_hll.value.add(user_hash)\n",
    "    return count != len(broadcast_hll.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(line):\n",
    "    parsed_line = user_agent_parser.Parse(line)\n",
    "\n",
    "    device = parsed_line['device']['family'].lower() \n",
    "\n",
    "    browser = parsed_line['user_agent']['family'].lower()\n",
    "\n",
    "    os = parsed_line['os']['family'].lower()\n",
    "    return [device, browser, os]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_devices(word):\n",
    "    return word==\"iphone\" or word==\"firefox\" or word==\"windows\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P7xaIlYtHUm3"
   },
   "outputs": [],
   "source": [
    "# Type your code for data processing and aggregation here\n",
    "\n",
    "\n",
    "dstream.filter(lambda line : filter_duplicate_user(line, hll))\\\n",
    "    .flatMap(lambda line: preprocess(line))\\\n",
    "    .filter(lambda word : filter_devices(word))\\\n",
    "    .map(lambda word: (word, 1))\\\n",
    "    .reduceByKey(lambda a,b : a+b)\\\n",
    "    .updateStateByKey(aggregator)\\\n",
    "    .foreachRDD(print_only_at_the_end)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vO7omMKEHUnD"
   },
   "source": [
    "**NB.** Please don't change the cell below. It is used for stopping SparkStreaming context and Spark context when the stream finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "puSwVJshHUnG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko2\n",
      "iphone\t1728\n",
      "firefox\t5622\n",
      "windows\t37295\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n",
      "kiko1\n"
     ]
    }
   ],
   "source": [
    "ssc.checkpoint('./checkpoint')  # checkpoint for storing current state        \n",
    "ssc.start()\n",
    "while not printed:\n",
    "    pass\n",
    "ssc.stop()  # when the result printed, stop the SparkStreaming context\n",
    "sc.stop()  # stop the Spark context to be able restart the code without restarting the kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GPMoQ7HyHUnJ"
   },
   "source": [
    "Here you can see the part of an output on the sample dataset:\n",
    "```\n",
    "...\n",
    "seg_unknown 22377\n",
    "seg_firefox 8237\n",
    "...\n",
    "```\n",
    "Of course, the numbers may be different but not very much (the error about 1% will be accepted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "1002_flavor.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
