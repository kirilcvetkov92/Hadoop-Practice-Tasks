{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salary prediction by vacancy description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset represents the data about vacancies which were published in the world net for different countries. The vacancy info has a full description of this vacancy, title, location, company, working category, salary etc.\n",
    "In this assignment you have to predict the possibility of raising the salary threshold, using the vacancy description. The data is presented in the dataframe. The columns of interest are:\n",
    "* FullDescription - description of vacancy\n",
    "* SalaryNormalized - predicted salary threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset description\n",
    "\n",
    "There are steps which are required to successfully complete the assignment:\n",
    "1. Read dataset\n",
    "2. Perform text transformation by removing punctuation terms and stop words.\n",
    "3. Generate n-grams.\t\n",
    "4. Count TF * IDF features\n",
    "5. Fit model for generated features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init pyspark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals # For the compatibility with Python 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark_session = SparkSession.builder\\\n",
    "                            .enableHiveSupport()\\\n",
    "                            .appName(\"spark sql\")\\\n",
    "                            .master(\"local[4]\")\\\n",
    "                            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls /data/vacancie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls /data/covertype2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = spark_session.read.csv(\"/data/covertype2/train.csv\",inferSchema=True,header=True)\n",
    "\n",
    "#train_data.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = train_data.select('FullDescription','SalaryNormalized')\n",
    "#train_data.select('Hillshade_3pm').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"Wild_Type\", outputCol=\"Wild_Type_Indexed\")\n",
    "model = stringIndexer.fit(train_data)\n",
    "indexed = model.transform(train_data)\n",
    "encoder = OneHotEncoder(inputCol=\"Wild_Type_Indexed\", outputCol=\"Wild_Type_Hot\")\n",
    "encoded = encoder.transform(indexed)\n",
    "\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"Soil_Type\", outputCol=\"Soil_Type_Indexed\")\n",
    "model = stringIndexer.fit(encoded)\n",
    "raw_data = model.transform(encoded)\n",
    "encoder = OneHotEncoder(inputCol=\"Soil_Type_Indexed\", outputCol=\"Soil_Type_Hot\")\n",
    "encoded = encoder.transform(raw_data)\n",
    "\n",
    "cols = list(set(encoded.columns) - {'Wild_Type', 'Soil_Type', 'Target', 'Soil_Type_Indexed', 'Wild_Type_Indexed' })\n",
    "print(len(cols))\n",
    "\n",
    "#encoded = encoded.select(cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Elevation: integer (nullable = true)\n",
      " |-- Aspect: integer (nullable = true)\n",
      " |-- Slope: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Vertical_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Roadways: integer (nullable = true)\n",
      " |-- Hillshade_9am: integer (nullable = true)\n",
      " |-- Hillshade_Noon: integer (nullable = true)\n",
      " |-- Hillshade_3pm: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Fire_Points: integer (nullable = true)\n",
      " |-- Wild_Type: string (nullable = true)\n",
      " |-- Soil_Type: string (nullable = true)\n",
      " |-- Target: integer (nullable = true)\n",
      " |-- Wild_Type_Indexed: double (nullable = true)\n",
      " |-- Wild_Type_Hot: vector (nullable = true)\n",
      " |-- Soil_Type_Indexed: double (nullable = true)\n",
      " |-- Soil_Type_Hot: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "from  pyspark.ml.feature  import VectorAssembler\n",
    "\n",
    "# Define assembler \n",
    "assembler = VectorAssembler(\n",
    "    inputCols=cols,\n",
    "    outputCol='features')\n",
    "\n",
    "# transform\n",
    "vector_indexed_data = assembler.transform(encoded)\n",
    "vector_indexed_data.printSchema()\n",
    "\n",
    "dataset = vector_indexed_data.select('features', 'Target')\n",
    "\n",
    "# scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "#                         withStd=True, withMean=False)\n",
    "\n",
    "# # Compute summary statistics by fitting the StandardScaler\n",
    "# scaler_model = scaler.fit(dataset)\n",
    "# scaledData = scaler_model.transform(dataset)\n",
    "# dataset = scaledData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------+------+\n",
      "|features                                                                                                     |Target|\n",
      "+-------------------------------------------------------------------------------------------------------------+------+\n",
      "|(52,[0,1,2,3,4,7,44,45,47,49,50,51],[195.0,266.0,245.0,3122.0,451.0,1.0,433.0,3069.0,1.0,188.0,10.0,75.0])   |1     |\n",
      "|(52,[0,1,2,3,4,5,44,45,46,49,50,51],[177.0,308.0,229.0,3018.0,4546.0,1.0,60.0,5359.0,1.0,192.0,15.0,14.0])   |1     |\n",
      "|(52,[0,1,2,3,4,5,44,45,46,49,50,51],[236.0,151.0,240.0,3146.0,1371.0,1.0,541.0,5887.0,1.0,132.0,12.0,-2.0])  |2     |\n",
      "|(52,[0,1,2,3,4,5,44,45,46,49,50,51],[226.0,163.0,242.0,2980.0,1087.0,1.0,553.0,3538.0,1.0,149.0,6.0,21.0])   |2     |\n",
      "|(52,[0,1,2,3,4,11,44,45,46,49,50,51],[220.0,187.0,250.0,2972.0,4119.0,1.0,255.0,6390.0,1.0,158.0,16.0,109.0])|2     |\n",
      "+-------------------------------------------------------------------------------------------------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show(5, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset to train and validation part (it is better to use 90% for the train part and 10% for the validation part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = dataset.randomSplit([0.9, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the Logistic Regression to the model on the splitted train part. Use about 15 iterations for the training process.\n",
    "\n",
    "<b>Hint.</b> Use regularization parameter in order to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr = LogisticRegression(featuresCol='features',labelCol='labels')\n",
    "\n",
    "lr = rf = RandomForestClassifier(labelCol=\"Target\", featuresCol=\"features\", numTrees=100, maxBins=40, maxDepth=7)\n",
    "\n",
    "model = lr.fit(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the loss function for each iteration. What can you notice from the behaviour of loss function?\n",
    "\n",
    "<b>Hint.</b> Use summary.objectiveHistory for this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainingSummary = model.summary\n",
    "\n",
    "# trainingSummary.predictions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the model to the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_results = model.evaluate(test_data)\n",
    "\n",
    "# test_results.predictions.show(5)\n",
    "\n",
    "#test_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary.objectiveHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test_data)\n",
    "\n",
    "#predictions.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate AUC-ROC for the predicted data. For this purpose, you can use BinaryClassificationEvaluator from ml.evaluation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7029411136504411"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='Target') #default=label not labels\n",
    "evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Self-check question:</b>\n",
    "\n",
    "1. Try to fit and predict model using pure words. Has the result changed?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Performing test submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the learned models to the test dataset.\n",
    "\n",
    "<b>Note!</b> The test dataset will be changed during the test phase. Your last cell output must be the output of the AUC-ROC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "root\n",
      " |-- Elevation: integer (nullable = true)\n",
      " |-- Aspect: integer (nullable = true)\n",
      " |-- Slope: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Vertical_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Roadways: integer (nullable = true)\n",
      " |-- Hillshade_9am: integer (nullable = true)\n",
      " |-- Hillshade_Noon: integer (nullable = true)\n",
      " |-- Hillshade_3pm: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Fire_Points: integer (nullable = true)\n",
      " |-- Wild_Type: string (nullable = true)\n",
      " |-- Soil_Type: string (nullable = true)\n",
      " |-- Target: integer (nullable = true)\n",
      " |-- Wild_Type_Indexed: double (nullable = true)\n",
      " |-- Wild_Type_Hot: vector (nullable = true)\n",
      " |-- Soil_Type_Indexed: double (nullable = true)\n",
      " |-- Soil_Type_Hot: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+--------------------+------+\n",
      "|            features|Target|\n",
      "+--------------------+------+\n",
      "|(52,[0,1,2,3,4,10...|     3|\n",
      "|(52,[0,1,2,3,4,5,...|     2|\n",
      "|(52,[0,1,2,3,4,7,...|     1|\n",
      "|(52,[0,1,2,3,4,29...|     2|\n",
      "|(52,[0,1,2,3,4,7,...|     2|\n",
      "|(52,[0,1,2,3,4,6,...|     1|\n",
      "|(52,[0,1,2,3,4,13...|     2|\n",
      "|(52,[0,1,2,3,4,20...|     2|\n",
      "|(52,[0,1,2,3,4,6,...|     2|\n",
      "|(52,[0,1,2,3,4,7,...|     1|\n",
      "|(52,[0,1,2,3,4,20...|     2|\n",
      "|(52,[0,1,2,3,4,13...|     2|\n",
      "|(52,[0,1,2,3,4,15...|     2|\n",
      "|(52,[0,1,2,3,4,14...|     2|\n",
      "|(52,[0,1,2,3,4,16...|     1|\n",
      "|(52,[0,1,2,3,4,7,...|     2|\n",
      "|(52,[0,1,2,3,4,6,...|     2|\n",
      "|(52,[0,1,2,3,4,7,...|     2|\n",
      "|(52,[0,1,2,3,4,6,...|     1|\n",
      "|(52,[0,1,2,3,4,7,...|     1|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = spark_session.read.csv(\"/data/covertype2/train.csv\",inferSchema=True,header=True)\n",
    "\n",
    "test_data_ = spark_session.read.csv(\"/data/covertype2/test.csv\",inferSchema=True,header=True)\n",
    "\n",
    "\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"Wild_Type\", outputCol=\"Wild_Type_Indexed\")\n",
    "modeler = stringIndexer.fit(train_data)  # :) ATTENTION \n",
    "indexed = modeler.transform(test_data_)\n",
    "encoder = OneHotEncoder(inputCol=\"Wild_Type_Indexed\", outputCol=\"Wild_Type_Hot\")\n",
    "encoded = encoder.transform(indexed)\n",
    "\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"Soil_Type\", outputCol=\"Soil_Type_Indexed\")\n",
    "modeler = stringIndexer.fit(train_data)  # :) ATTENTION \n",
    "raw_data = modeler.transform(encoded)\n",
    "encoder = OneHotEncoder(inputCol=\"Soil_Type_Indexed\", outputCol=\"Soil_Type_Hot\")\n",
    "encoded = encoder.transform(raw_data)\n",
    "\n",
    "cols = list(set(encoded.columns) - {'Wild_Type', 'Soil_Type', 'Target', 'Soil_Type_Indexed', 'Wild_Type_Indexed'})\n",
    "\n",
    "print(len(cols))\n",
    "\n",
    "# Define assembler \n",
    "# Define assembler \n",
    "assembler = VectorAssembler(\n",
    "    inputCols=cols,\n",
    "    outputCol='features')\n",
    "\n",
    "# transform\n",
    "vector_indexed_data = assembler.transform(encoded)\n",
    "vector_indexed_data.printSchema()\n",
    "\n",
    "dataset = vector_indexed_data.select('features', 'Target')\n",
    "\n",
    "dataset.show()\n",
    "# scaler = StandardScaler(inputCol=\"features\", outputCol=\"sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------+------+\n",
      "|features                                                                                                    |Target|\n",
      "+------------------------------------------------------------------------------------------------------------+------+\n",
      "|(52,[0,1,2,3,4,10,44,45,48,49,50,51],[197.0,44.0,125.0,2025.0,216.0,1.0,95.0,124.0,1.0,30.0,40.0,75.0])     |3     |\n",
      "|(52,[0,1,2,3,4,5,44,45,46,49,50,51],[226.0,52.0,197.0,3092.0,2408.0,1.0,182.0,4425.0,1.0,98.0,18.0,39.0])   |2     |\n",
      "|(52,[0,1,2,3,4,7,44,45,47,49,50,51],[188.0,346.0,213.0,3403.0,1254.0,1.0,698.0,2505.0,1.0,163.0,16.0,111.0])|1     |\n",
      "+------------------------------------------------------------------------------------------------------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform dataset and calculate auc-roc\n",
    "dataset.show(3,False)\n",
    "predictions_2 = model.transform(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7052542460735698"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predictions_2, {evaluator.metricName: \"accuracy\"})"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
