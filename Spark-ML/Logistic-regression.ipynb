{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salary prediction by vacancy description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset represents the data about vacancies which were published in the world net for different countries. The vacancy info has a full description of this vacancy, title, location, company, working category, salary etc.\n",
    "In this assignment you have to predict the possibility of raising the salary threshold, using the vacancy description. The data is presented in the dataframe. The columns of interest are:\n",
    "* FullDescription - description of vacancy\n",
    "* SalaryNormalized - predicted salary threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset description\n",
    "\n",
    "There are steps which are required to successfully complete the assignment:\n",
    "1. Read dataset\n",
    "2. Perform text transformation by removing punctuation terms and stop words.\n",
    "3. Generate n-grams.\t\n",
    "4. Count TF * IDF features\n",
    "5. Fit model for generated features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init pyspark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals # For the compatibility with Python 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark_session = SparkSession.builder\\\n",
    "                            .enableHiveSupport()\\\n",
    "                            .appName(\"spark sql\")\\\n",
    "                            .master(\"local[4]\")\\\n",
    "                            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.csv  train.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls /data/vacancie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- FullDescription: string (nullable = true)\n",
      " |-- LocationRaw: string (nullable = true)\n",
      " |-- LocationNormalized: string (nullable = true)\n",
      " |-- ContractType: string (nullable = true)\n",
      " |-- ContractTime: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- SalaryRaw: string (nullable = true)\n",
      " |-- SalaryNormalized: integer (nullable = true)\n",
      " |-- SourceName: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = spark_session.read.csv(\"/data/vacancie/\",inferSchema=True,header=True)\n",
    "\n",
    "train_data.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+\n",
      "|     FullDescription|SalaryNormalized|\n",
      "+--------------------+----------------+\n",
      "|Stress Engineer G...|               0|\n",
      "|Mathematical Mode...|               0|\n",
      "|Engineering Syste...|               0|\n",
      "|Pioneer, Miser  E...|               0|\n",
      "|Engineering Syste...|               0|\n",
      "|This is an except...|               0|\n",
      "|A subsea engineer...|               1|\n",
      "|Are you a success...|               0|\n",
      "|PROJECT ENGINEER ...|               1|\n",
      "|Senior Fatigue St...|               1|\n",
      "|A well respected ...|               0|\n",
      "|Our client are a ...|               0|\n",
      "|A leading Subsea ...|               1|\n",
      "|A popular hotel l...|               0|\n",
      "| HOTEL AND CONFER...|               0|\n",
      "|Senior Control an...|               1|\n",
      "|Control and Instr...|               1|\n",
      "|Senior Process En...|               1|\n",
      "|CHEF DE PARTIE PO...|               0|\n",
      "|Senior Sous Chef ...|               0|\n",
      "+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = train_data.select('FullDescription','SalaryNormalized')\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove redundant punctuation signs using RegexTokenizer with pattern <code>\"\\\\\\\\s+|,|\\\\\\\\*|/|\\\\\\\\.\"</code>. This pattern removes whitespaces, commas, dots and other characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+------------------------+\n",
      "|     FullDescription|SalaryNormalized|FullDescriptionTokenized|\n",
      "+--------------------+----------------+------------------------+\n",
      "|Stress Engineer G...|               0|    [stress, engineer...|\n",
      "|Mathematical Mode...|               0|    [mathematical, mo...|\n",
      "|Engineering Syste...|               0|    [engineering, sys...|\n",
      "+--------------------+----------------+------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "regexTokenizer = RegexTokenizer(pattern=\"\\\\s+|,|\\\\*|/|\\\\.\", inputCol=\"FullDescription\", outputCol=\"FullDescriptionTokenized\")\n",
    "regex_data = regexTokenizer.transform(dataset)\n",
    "\n",
    "print(regex_data.show(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove English stop words using StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol=\"FullDescriptionTokenized\", outputCol=\"FullDescriptionFiltered\")\n",
    "filtered_data = remover.transform(regex_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              2grams|\n",
      "+--------------------+\n",
      "|[stress engineer,...|\n",
      "|[mathematical mod...|\n",
      "|[engineering syst...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------------------+\n",
      "|              3grams|\n",
      "+--------------------+\n",
      "|[stress engineer ...|\n",
      "|[mathematical mod...|\n",
      "|[engineering syst...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------------------+----------------+------------------------+-----------------------+--------------------+--------------------+\n",
      "|     FullDescription|SalaryNormalized|FullDescriptionTokenized|FullDescriptionFiltered|              2grams|              3grams|\n",
      "+--------------------+----------------+------------------------+-----------------------+--------------------+--------------------+\n",
      "|Stress Engineer G...|               0|    [stress, engineer...|   [stress, engineer...|[stress engineer,...|[stress engineer ...|\n",
      "|Mathematical Mode...|               0|    [mathematical, mo...|   [mathematical, mo...|[mathematical mod...|[mathematical mod...|\n",
      "|Engineering Syste...|               0|    [engineering, sys...|   [engineering, sys...|[engineering syst...|[engineering syst...|\n",
      "+--------------------+----------------+------------------------+-----------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "\n",
    "ngram = NGram(n=2, inputCol=\"FullDescriptionFiltered\", outputCol=\"2grams\")\n",
    "\n",
    "ngramDataFrame = ngram.transform(filtered_data)\n",
    "ngramDataFrame.select(\"2grams\").show(3)\n",
    "\n",
    "ngram = NGram(n=3, inputCol=\"FullDescriptionFiltered\", outputCol=\"3grams\")\n",
    "\n",
    "ngramDataFrame = ngram.transform(ngramDataFrame)\n",
    "ngramDataFrame.select(\"3grams\").show(3)\n",
    "\n",
    "ngramDataFrame.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate n-grams with $n = 2$, $n = 3$ (module pyspark.ml.feature). After that you can perform some experiments with concatenating of column datasets (e.g. words and 3-grams or words with 2-grams and 3-grams). You can use the function in the cell below to concatenate lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def concat(type):\n",
    "    def concat_(*args):\n",
    "        return list(chain(*args))\n",
    "    return udf(concat_, ArrayType(type))\n",
    "\n",
    "concat_string_arrays = concat(StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+\n",
      "|            features|SalaryNormalized|\n",
      "+--------------------+----------------+\n",
      "|[stress engineer,...|               0|\n",
      "|[mathematical mod...|               0|\n",
      "|[engineering syst...|               0|\n",
      "|[pioneer miser, m...|               0|\n",
      "|[engineering syst...|               0|\n",
      "|[exceptional oppo...|               0|\n",
      "|[subsea engineeri...|               1|\n",
      "|[successful resul...|               0|\n",
      "|[project engineer...|               1|\n",
      "|[senior fatigue, ...|               1|\n",
      "+--------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngram = ngramDataFrame.select(concat_string_arrays(col(\"2grams\"),col(\"3grams\")).alias(\"features\"), \"SalaryNormalized\")\n",
    "\n",
    "ngram.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting TF-IDF features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use hashing trick and IDF to count features of train dataset. The appropriate features number for the dataset is about 2000. You can experiment with varying the number of features.\n",
    "\n",
    "\n",
    "<b>Note.</b> Remember to save IDF model in order to apply it to the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Hashing TF\n",
    "\n",
    "hashingTF = HashingTF()\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"features\", outputCol=\"rawFeatures\", numFeatures=400)\n",
    "featurizedData = hashingTF.transform(ngram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+\n",
      "|         rawFeatures|SalaryNormalized|\n",
      "+--------------------+----------------+\n",
      "|(400,[3,4,6,7,11,...|               1|\n",
      "|(400,[3,6,9,12,17...|               1|\n",
      "|(400,[1,10,16,17,...|               0|\n",
      "|(400,[2,10,12,13,...|               0|\n",
      "|(400,[1,7,14,16,2...|               1|\n",
      "|(400,[0,2,3,4,6,9...|               1|\n",
      "|(400,[0,1,4,10,13...|               0|\n",
      "|(400,[1,2,3,5,9,1...|               0|\n",
      "|(400,[1,3,4,5,6,8...|               1|\n",
      "|(400,[0,1,2,3,4,5...|               0|\n",
      "+--------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurizedData  = featurizedData.select(\"rawFeatures\", \"SalaryNormalized\")\n",
    "featurizedData.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data with the IDF model\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaledData = rescaledData.select(rescaledData.SalaryNormalized.alias(\"labels\"), \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset to train and validation part (it is better to use 90% for the train part and 10% for the validation part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = rescaledData.randomSplit([0.9, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the Logistic Regression to the model on the splitted train part. Use about 15 iterations for the training process.\n",
    "\n",
    "<b>Hint.</b> Use regularization parameter in order to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol='features',labelCol='labels')\n",
    "\n",
    "model = lr.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the loss function for each iteration. What can you notice from the behaviour of loss function?\n",
    "\n",
    "<b>Hint.</b> Use summary.objectiveHistory for this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------------------+----------+\n",
      "|labels|            features|       rawPrediction|         probability|prediction|\n",
      "+------+--------------------+--------------------+--------------------+----------+\n",
      "|   0.0|(400,[0,1,2,3,4,5...|[-1.3020942812736...|[0.21381276384264...|       1.0|\n",
      "|   0.0|(400,[0,1,2,3,4,5...|[-1.9345402186152...|[0.12624889764235...|       1.0|\n",
      "|   0.0|(400,[0,1,2,3,4,5...|[-0.0779602543740...|[0.48051980180497...|       1.0|\n",
      "|   0.0|(400,[0,1,2,3,4,5...|[0.96634800360846...|[0.72439097931260...|       0.0|\n",
      "|   0.0|(400,[0,1,2,3,4,5...|[0.91479487133508...|[0.71398033879609...|       0.0|\n",
      "+------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingSummary = model.summary\n",
    "\n",
    "trainingSummary.predictions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the model to the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------------------+----------+\n",
      "|labels|            features|       rawPrediction|         probability|prediction|\n",
      "+------+--------------------+--------------------+--------------------+----------+\n",
      "|     0|(400,[0,1,2,3,4,5...|[-0.7818075866801...|[0.31393044056371...|       1.0|\n",
      "|     0|(400,[0,1,2,3,4,5...|[-0.1445073275075...|[0.46393590493266...|       1.0|\n",
      "|     0|(400,[0,1,2,3,4,5...|[0.86611700775817...|[0.70393708623075...|       0.0|\n",
      "|     0|(400,[0,1,2,3,4,5...|[-0.9014196299617...|[0.28875885037011...|       1.0|\n",
      "|     0|(400,[0,1,2,3,4,5...|[0.62730115244500...|[0.65187725512390...|       0.0|\n",
      "+------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results = model.evaluate(test_data)\n",
    "\n",
    "test_results.predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6927536511049203,\n",
       " 0.691332843856734,\n",
       " 0.6906553500872187,\n",
       " 0.6896795432297079,\n",
       " 0.6865647069577447,\n",
       " 0.6800505984184066,\n",
       " 0.6674675316069572,\n",
       " 0.6522705972539948,\n",
       " 0.6426301234840726,\n",
       " 0.6404475468547574,\n",
       " 0.6401993871264021,\n",
       " 0.6400408063473533,\n",
       " 0.639693774409343,\n",
       " 0.6389731722208523,\n",
       " 0.637731206551552,\n",
       " 0.6371392086968888,\n",
       " 0.635856429727614,\n",
       " 0.6357584855198875,\n",
       " 0.6357076989626469,\n",
       " 0.6356138794759065,\n",
       " 0.635336738935395,\n",
       " 0.6351580612111113,\n",
       " 0.6350410210005988,\n",
       " 0.6350080574849847,\n",
       " 0.6349639728371852,\n",
       " 0.6349284791445033,\n",
       " 0.6348704224118138,\n",
       " 0.6347874068662419,\n",
       " 0.6347583778647272,\n",
       " 0.634704543474558,\n",
       " 0.6346942011585923,\n",
       " 0.6346864378022077,\n",
       " 0.6346748326989902,\n",
       " 0.6346621482583705,\n",
       " 0.6346569260888605,\n",
       " 0.6346497683876172,\n",
       " 0.634649255021077,\n",
       " 0.6346488246559323,\n",
       " 0.6346481327206955,\n",
       " 0.6346476019175346,\n",
       " 0.6346475819913922,\n",
       " 0.6346471995709722,\n",
       " 0.6346470460722691,\n",
       " 0.6346469452903982,\n",
       " 0.6346468715448711,\n",
       " 0.6346467225114841,\n",
       " 0.6346466765821948,\n",
       " 0.6346465500950841,\n",
       " 0.6346464987746153,\n",
       " 0.6346463320709536,\n",
       " 0.6346461900237407,\n",
       " 0.6346461702755596,\n",
       " 0.6346461167728317,\n",
       " 0.6346461139275801,\n",
       " 0.6346460959377511,\n",
       " 0.6346460933356407,\n",
       " 0.6346460714520362,\n",
       " 0.634646067303973,\n",
       " 0.6346460643716066,\n",
       " 0.6346460602150951,\n",
       " 0.6346460576119453,\n",
       " 0.6346460553024639,\n",
       " 0.6346460552526452,\n",
       " 0.6346460550307799,\n",
       " 0.6346460549106788]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary.objectiveHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------------------+----------+\n",
      "|labels|            features|       rawPrediction|         probability|prediction|\n",
      "+------+--------------------+--------------------+--------------------+----------+\n",
      "|     0|(400,[0,1,2,3,4,5...|[-0.7818075866801...|[0.31393044056371...|       1.0|\n",
      "|     0|(400,[0,1,2,3,4,5...|[-0.1445073275075...|[0.46393590493266...|       1.0|\n",
      "|     0|(400,[0,1,2,3,4,5...|[0.86611700775817...|[0.70393708623075...|       0.0|\n",
      "+------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test_data)\n",
    "\n",
    "predictions.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate AUC-ROC for the predicted data. For this purpose, you can use BinaryClassificationEvaluator from ml.evaluation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC 0.6559214041310016\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='labels') #default=label not labels\n",
    "print('Test Area Under ROC', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Self-check question:</b>\n",
    "\n",
    "1. Try to fit and predict model using pure words. Has the result changed?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Performing test submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the learned models to the test dataset.\n",
    "\n",
    "<b>Note!</b> The test dataset will be changed during the test phase. Your last cell output must be the output of the AUC-ROC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = spark_session.read.csv(\"/data/vacancie/test.csv\",inferSchema=True,header=True)\n",
    "test_dataset = test_data.select('FullDescription','SalaryNormalized')\n",
    "regexTokenizer = RegexTokenizer(pattern=\"\\\\s+|,|\\\\*|/|\\\\.\", inputCol=\"FullDescription\", outputCol=\"FullDescriptionTokenized\")\n",
    "test_regex_data = regexTokenizer.transform(test_dataset)\n",
    "remover = StopWordsRemover(inputCol=\"FullDescriptionTokenized\", outputCol=\"FullDescriptionFiltered\")\n",
    "test_filtered_data = remover.transform(test_regex_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ngram = NGram(n=2, inputCol=\"FullDescriptionFiltered\", outputCol=\"2grams\")\n",
    "\n",
    "ngramDataFrame = ngram.transform(test_filtered_data)\n",
    "\n",
    "ngram = NGram(n=3, inputCol=\"FullDescriptionFiltered\", outputCol=\"3grams\")\n",
    "\n",
    "ngramDataFrame = ngram.transform(ngramDataFrame)\n",
    "\n",
    "ngram = ngramDataFrame.select(concat_string_arrays(col(\"2grams\"),col(\"3grams\")).alias(\"features\"), \"SalaryNormalized\")\n",
    "\n",
    "hashingTF = HashingTF()\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"features\", outputCol=\"rawFeatures\", numFeatures=400)\n",
    "featurizedData = hashingTF.transform(ngram)\n",
    "\n",
    "\n",
    "test_filtered_data = featurizedData.select(featurizedData.rawFeatures.alias('features'), \n",
    "                                              featurizedData.SalaryNormalized.alias('labels'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            features|labels|\n",
      "+--------------------+------+\n",
      "|(400,[3,4,6,7,11,...|     1|\n",
      "|(400,[3,6,9,12,17...|     1|\n",
      "|(400,[1,10,16,17,...|     0|\n",
      "|(400,[2,10,12,13,...|     0|\n",
      "|(400,[1,7,14,16,2...|     1|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform dataset and calculate auc-roc\n",
    "test_filtered_data.show(5)\n",
    "\n",
    "test_results_dataset = model.evaluate(test_filtered_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6872518365175885\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.evaluate(test_results_dataset.predictions))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
